Release "kubescape" has been upgraded. Happy Helming!
NAME: kubescape
LAST DEPLOYED: Wed May 24 14:49:56 2023
NAMESPACE: kubescape
STATUS: pending-upgrade
REVISION: 3
TEST SUITE: None
USER-SUPPLIED VALUES:
account: 2ce5daf4-e28d-4e6e-a239-03fda048070b
addRevisionLabel: true
clusterName: arn:aws:eks:eu-west-2:015253967648:cluster/matan_avital-test-mjrwe
environment: dev
global:
  httpsProxy: https://nginx-proxy.default:443

COMPUTED VALUES:
account: 2ce5daf4-e28d-4e6e-a239-03fda048070b
addRevisionLabel: true
backendOpenAPI: https://api.armosec.io/api
clientID: ""
cloudProviderMetadata:
  aksResourceGroup: null
  aksSubscriptionID: null
  awsIamRoleArn: null
  cloudProviderEngine: null
  cloudRegion: null
  gkeProject: null
  gkeServiceAccount: null
clusterName: arn:aws:eks:eu-west-2:015253967648:cluster/matan_avital-test-mjrwe
createKubescapeServiceAccount: true
devBackendOpenAPI: https://api-dev.armosec.io/api
devEventReceiverHttpUrl: https://report.eudev3.cyberarmorsoft.com
devGatewayUrl: ens.eudev3.cyberarmorsoft.com
devK8sReportUrl: wss://report.eudev3.cyberarmorsoft.com
devKsCloudOtelCollector: otelcol-dev.armosec.io:443
environment: dev
eventReceiverHttpUrl: https://report.armo.cloud
gateway:
  enabled: true
  env: {}
  httpService:
    port: 8002
    protocol: TCP
    targetPort: 8002
    type: ClusterIP
  image:
    pullPolicy: IfNotPresent
    repository: quay.io/kubescape/gateway
    tag: v0.1.11
  labels: {}
  name: gateway
  replicaCount: 1
  resources:
    limits:
      cpu: 100m
      memory: 50Mi
    requests:
      cpu: 10m
      memory: 10Mi
  volumeMounts: []
  volumes: []
  websocketService:
    port: 8001
    protocol: TCP
    targetPort: 8001
    type: ClusterIP
gatewayUrl: ens.euprod1.cyberarmorsoft.com
global:
  cloudConfig: ks-cloud-config
  httpsProxy: https://nginx-proxy.default:443
  kubescapePsp:
    enabled: false
    name: ks-allow-privileged
  kubescapeServiceAccountName: kubescape-sa
  namespaceTier: ks-control-plane
  networkPolicy:
    createEgressRules: false
    enabled: false
  operatorServiceAccountName: ks-sa
imagePullSecrets: ""
k8sReportUrl: wss://report.armo.cloud
kollector:
  enabled: true
  env:
  - name: PRINT_REPORT
    value: "false"
  - name: WAIT_BEFORE_REPORT
    value: "0"
  image:
    pullPolicy: Always
    repository: quay.io/kubescape/kollector
    tag: v0.1.16
  labels: {}
  name: kollector
  replicaCount: 1
  resources:
    limits:
      cpu: 500m
      memory: 500Mi
    requests:
      cpu: 10m
      memory: 40Mi
  volumeMounts: []
  volumes: []
ksCloudOtelCollector: otelcol.armosec.io:443
ksLabel: kubescape
ksNamespace: kubescape
kubescape:
  downloadArtifacts: true
  enableHostScan: true
  enabled: true
  image:
    pullPolicy: Always
    repository: quay.io/kubescape/kubescape
    tag: v2.3.3
  name: kubescape
  replicaCount: 1
  resources:
    limits:
      cpu: 600m
      memory: 800Mi
    requests:
      cpu: 250m
      memory: 400Mi
  ruleProcessingConcurrency: 1
  service:
    port: 8080
    type: ClusterIP
  serviceMonitor:
    enabled: false
  skipUpdateCheck: false
  submit: true
  volumeMounts: []
  volumes: []
kubescapeHostScanner:
  volumeMounts: []
  volumes: []
kubescapeScheduler:
  enabled: true
  image:
    pullPolicy: IfNotPresent
    repository: quay.io/kubescape/http-request
    tag: v0.0.14
  name: kubescape-scheduler
  replicaCount: 1
  resources:
    limits:
      cpu: 10m
      memory: 20Mi
    requests:
      cpu: 1m
      memory: 10Mi
  scanSchedule: 0 8 * * *
  volumeMounts: []
  volumes: []
kubevuln:
  enabled: true
  env:
  - name: CA_MAX_VULN_SCAN_ROUTINES
    value: "1"
  image:
    pullPolicy: Always
    repository: quay.io/kubescape/kubevuln
    tag: v0.1.19
  labels: {}
  name: kubevuln
  replicaCount: 1
  resources:
    limits:
      cpu: 500m
      memory: 5000Mi
    requests:
      cpu: 300m
      ephemeral-storage: 5Gi
      memory: 1000Mi
  service:
    port: 8080
    protocol: TCP
    targetPort: 8080
    type: ClusterIP
  verbose: ""
  volumeMounts: []
  volumes: []
kubevulnScheduler:
  enabled: true
  image:
    pullPolicy: IfNotPresent
    repository: quay.io/kubescape/http-request
    tag: v0.0.14
  name: kubevuln-scheduler
  replicaCount: 1
  resources:
    limits:
      cpu: 10m
      memory: 20Mi
    requests:
      cpu: 1m
      memory: 10Mi
  scanSchedule: 0 0 * * *
  volumeMounts: []
  volumes: []
logger:
  level: info
  name: zap
operator:
  enabled: true
  env: {}
  image:
    pullPolicy: Always
    repository: quay.io/kubescape/operator
    tag: v0.1.18
  labels: {}
  name: operator
  replicaCount: 1
  resources:
    limits:
      cpu: 300m
      memory: 300Mi
    requests:
      cpu: 50m
      memory: 100Mi
  service:
    port: 4002
    protocol: TCP
    targetPort: 4002
    type: ClusterIP
  volumeMounts: []
  volumes: []
otelCollector:
  enabled: true
  endpoint:
    headers:
      uptrace-dsn: ""
    host: ""
    insecure: true
    port: 4317
  hostmetrics:
    enabled: false
    scrapeInterval: 30s
  image:
    pullPolicy: Always
    repository: otel/opentelemetry-collector
    tag: 0.70.0
  replicaCount: 1
  resources:
    limits:
      cpu: 1
      memory: 1Gi
    requests:
      cpu: 100m
      memory: 200Mi
registryScanScheduler:
  enabled: true
  image:
    pullPolicy: IfNotPresent
    repository: quay.io/kubescape/http-request
    tag: v0.0.14
  name: registry-scheduler
  replicaCount: 1
  resources:
    limits:
      cpu: 10m
      memory: 20Mi
    requests:
      cpu: 1m
      memory: 10Mi
  scanSchedule: 0 0 * * *
  volumeMounts: []
  volumes: []
secretKey: ""
stagingBackendOpenAPI: https://api-stage.armosec.io/api
stagingEventReceiverHttpUrl: https://report-ks.eustage2.cyberarmorsoft.com
stagingGatewayUrl: ens.eustage2.cyberarmorsoft.com
stagingK8sReportUrl: wss://report.eustage2.cyberarmorsoft.com
triggerNewImageScan: false
volumeMounts: []
volumes: []

HOOKS:
MANIFEST:
---
# Source: kubescape-cloud-operator/templates/kubescape/serviceaccount.yaml
kind: ServiceAccount
apiVersion: v1
metadata:
  labels:
    app: kubescape
  name: kubescape-sa
  namespace: kubescape
automountServiceAccountToken: false
---
# Source: kubescape-cloud-operator/templates/kv-service-account.yaml
kind: ServiceAccount
apiVersion: v1
metadata:
  labels:
    app: kubescape
  name: ks-sa
  namespace: kubescape
automountServiceAccountToken: false
---
# Source: kubescape-cloud-operator/templates/configs/cloudapi-configmap.yaml
kind: ConfigMap 
apiVersion: v1 
metadata:
  name: ks-cloud-config
  namespace: kubescape
  labels:
    app: ks-cloud-config
    tier: ks-control-plane
data:
  clusterData: |
    {
      "gatewayWebsocketURL": "gateway:8001",
      "gatewayRestURL": "gateway:8002",
      "vulnScanURL": "kubevuln:8080",
      "kubevulnURL": "kubevuln:8080",
      "kubescapeURL": "kubescape:8080",
      "triggerNewImageScan": "false",
      "accountID": "2ce5daf4-e28d-4e6e-a239-03fda048070b",
      "clusterName": "arn-aws-eks-eu-west-2-015253967648-cluster-matan_avital-test-mjrwe", 
      "backendOpenAPI": "https://api-dev.armosec.io/api",
      "eventReceiverRestURL": "https://report.eudev3.cyberarmorsoft.com",
      "eventReceiverWebsocketURL": "wss://report.eudev3.cyberarmorsoft.com",
      "rootGatewayURL": "wss://ens.eudev3.cyberarmorsoft.com/v1/waitfornotification"       
    }
---
# Source: kubescape-cloud-operator/templates/configs/host-scanner-definition-configmap.yaml
kind: ConfigMap 
apiVersion: v1 
metadata:
  name: host-scanner-definition
  namespace: kubescape
  labels:
    app: ks-cloud-config
    tier: ks-control-plane
data:
  host-scanner-yaml: |-
    
    apiVersion: v1
    kind: Namespace
    metadata:
      labels:
        app: kubescape-host-scanner
        k8s-app: kubescape-host-scanner
        kubernetes.io/metadata.name: kubescape-host-scanner
        tier: kubescape-host-scanner-control-plane
      name: kubescape-host-scanner
    ---
    apiVersion: apps/v1
    kind: DaemonSet
    metadata:
      name: host-scanner
      namespace: kubescape-host-scanner
      labels:
        app: host-scanner
        k8s-app: kubescape-host-scanner
    spec:
      selector:
        matchLabels:
          name: host-scanner
      template:
        metadata:
          labels:
            name: host-scanner
            otel: enabled
        spec:
          tolerations:
          # this toleration is to have the DaemonDet runnable on master nodes
          # remove it if your masters can't run pods
          - key: node-role.kubernetes.io/control-plane
            operator: Exists
            effect: NoSchedule
          - key: node-role.kubernetes.io/master
            operator: Exists
            effect: NoSchedule
          containers:
          - name: host-sensor
            image: quay.io/kubescape/host-scanner:v1.0.54
            securityContext:
              privileged: true
              readOnlyRootFilesystem: true
              procMount: Unmasked
            env:
            - name: KS_LOGGER_LEVEL
              value: "info"
            - name: KS_LOGGER_NAME
              value: "zap"
            - name: ACCOUNT_ID
              value: "2ce5daf4-e28d-4e6e-a239-03fda048070b"
            - name: CLUSTER_NAME
              value: "arn-aws-eks-eu-west-2-015253967648-cluster-matan_avital-test-mjrwe"
            - name: OTEL_COLLECTOR_SVC
              value: "otel-collector.kubescape.svc:4317"
            ports:
              - name: scanner # Do not change port name
                containerPort: 7888
                protocol: TCP
            resources:
              limits:
                cpu: 0.4m
                memory: 400Mi
              requests:
                cpu: 0.1m
                memory: 200Mi
            volumeMounts:
            - mountPath: /host_fs
              name: host-filesystem
            readinessProbe:
              httpGet:
                path: /kernelVersion
                port: 7888
                initialDelaySeconds: 1
                periodSeconds: 1
          terminationGracePeriodSeconds: 120
          dnsPolicy: ClusterFirstWithHostNet
          automountServiceAccountToken: false
          volumes:
          - hostPath:
              path: /
              type: Directory
            name: host-filesystem
          hostPID: true
          hostIPC: true
---
# Source: kubescape-cloud-operator/templates/configs/ks-recurring-cronjob-configmap.yaml
kind: ConfigMap 
apiVersion: v1 
metadata:
  name: kubescape-cronjob-template
  namespace: kubescape
  labels:
    app: ks-cloud-config
    tier: ks-control-plane
data:
  cronjobTemplate: |-
    apiVersion: batch/v1
    kind: CronJob
    metadata:
      name: kubescape-scheduler
      namespace: kubescape
      labels:
        app: kubescape-scheduler
        tier: ks-control-plane
        armo.tier: "kubescape-scan"
    spec:
      schedule: "0 8 * * *"
      jobTemplate:
        spec:
          template:
            metadata:
              labels:
                armo.tier: "kubescape-scan"
            spec:
              containers:
              - name: kubescape-scheduler
                image: "quay.io/kubescape/http-request:v0.0.14"
                imagePullPolicy: IfNotPresent
                securityContext:
                  allowPrivilegeEscalation: false
                  readOnlyRootFilesystem: true
                  runAsNonRoot: true
                  runAsUser: 100
                resources:
                  limits:
                    cpu: 10m
                    memory: 20Mi
                  requests:
                    cpu: 1m
                    memory: 10Mi
                args: 
                  - -method=post
                  - -scheme=http
                  - -host=operator:4002
                  - -path=v1/triggerAction
                  - -headers="Content-Type:application/json"
                  - -path-body=/home/ks/request-body.json
                volumeMounts:
                  - name: "request-body-volume"
                    mountPath: /home/ks/request-body.json
                    subPath: request-body.json
                    readOnly: true
              restartPolicy: Never
              automountServiceAccountToken: false
              volumes:
                - name: "request-body-volume" # placeholder
                  configMap:
                    name: kubescape-scheduler
---
# Source: kubescape-cloud-operator/templates/configs/kv-recurring-cronjob-configmap.yaml
kind: ConfigMap 
apiVersion: v1 
metadata:
  name: kubevuln-cronjob-template # TODO: update template name
  namespace: kubescape
  labels:
    app: ks-cloud-config
    tier: ks-control-plane
data:
  cronjobTemplate: |-
    apiVersion: batch/v1
    kind: CronJob
    metadata:
      name: kubevuln-scheduler
      namespace: kubescape
      labels:
        app: kubevuln-scheduler
        tier: ks-control-plane
        armo.tier: "vuln-scan"
    spec:
      schedule: "0 0 * * *" 
      jobTemplate:
        spec:
          template:
            metadata:
              labels:
                armo.tier: "vuln-scan"
            spec:
              containers:
              - name: kubevuln-scheduler
                image: "quay.io/kubescape/http-request:v0.0.14"
                imagePullPolicy: IfNotPresent
                securityContext:
                  allowPrivilegeEscalation: false
                  readOnlyRootFilesystem: true
                  runAsNonRoot: true
                  runAsUser: 100
                resources:
                  limits:
                    cpu: 10m
                    memory: 20Mi
                  requests:
                    cpu: 1m
                    memory: 10Mi
                args: 
                  - -method=post
                  - -scheme=http
                  - -host=operator:4002
                  - -path=v1/triggerAction
                  - -headers="Content-Type:application/json"
                  - -path-body=/home/ks/request-body.json
                volumeMounts:
                  - name: "request-body-volume"
                    mountPath: /home/ks/request-body.json
                    subPath: request-body.json
                    readOnly: true
              restartPolicy: Never
              automountServiceAccountToken: false
              volumes:
                - name: "request-body-volume" # placeholder
                  configMap:
                    name: kubevuln-scheduler
---
# Source: kubescape-cloud-operator/templates/configs/otel-collector-configmap.yaml
kind: ConfigMap 
apiVersion: v1 
metadata:
  name: otel-collector-config
  namespace: kubescape
  labels:
    app: ks-cloud-config
    tier: ks-control-plane
data:
  otel-collector-config.yaml: |-
    
    # receivers configure how data gets into the Collector.
    receivers:
      otlp:
        protocols:
          grpc:
          http:
      hostmetrics:
        collection_interval: 30s
        scrapers:
          cpu:
          memory:
    
    # processors specify what happens with the received data.
    processors:
      attributes/ksCloud:
        actions:
          - key: account_id
            value: "2ce5daf4-e28d-4e6e-a239-03fda048070b"
            action: upsert
          - key: cluster_name
            value: "arn-aws-eks-eu-west-2-015253967648-cluster-matan_avital-test-mjrwe"
            action: upsert
      batch:
        send_batch_size: 10000
        timeout: 10s
    
    # exporters configure how to send processed data to one or more backends.
    exporters:
      otlp/ksCloud:
        endpoint: "otelcol-dev.armosec.io:443"
        tls:
          insecure: false
    
    # service pulls the configured receivers, processors, and exporters together into
    # processing pipelines. Unused receivers/processors/exporters are ignored.
    service:
      pipelines:
        traces:
          receivers: [otlp]
          processors: [batch]
          exporters:
            - otlp/ksCloud
        metrics:
          receivers: [otlp]
          processors: [batch]
          exporters:
            - otlp/ksCloud
        logs:
          receivers: [otlp]
          processors: [batch]
          exporters:
            - otlp/ksCloud
---
# Source: kubescape-cloud-operator/templates/configs/registry-scan-recurring-cronjob-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  namespace: kubescape
  labels:
    app: ks-cloud-config
    tier: ks-control-plane
  name: registry-scan-cronjob-template
data:
  cronjobTemplate: |-
    apiVersion: batch/v1
    kind: CronJob
    metadata:
      name: registry-scheduler
      namespace: kubescape
      labels:
        app: registry-scheduler
        tier: ks-control-plane
        armo.tier: "registry-scan"
    spec:
      schedule: "0 0 * * *"
      jobTemplate:
        spec:
          template:
            metadata:
              labels:
                armo.tier: "registry-scan"
            spec:
              containers:
              - name: registry-scheduler
                image: "quay.io/kubescape/http-request:v0.0.14"
                imagePullPolicy: IfNotPresent
                securityContext:
                  allowPrivilegeEscalation: false
                  readOnlyRootFilesystem: true
                  runAsNonRoot: true
                  runAsUser: 100
                resources:
                  limits:
                    cpu: 10m
                    memory: 20Mi
                  requests:
                    cpu: 1m
                    memory: 10Mi
                args: 
                  - -method=post
                  - -scheme=http
                  - -host=operator:4002
                  - -path=v1/triggerAction
                  - -headers="Content-Type:application/json"
                  - -path-body=/home/ks/request-body.json
                volumeMounts:
                  - name: "request-body-volume"
                    mountPath: /home/ks/request-body.json
                    subPath: request-body.json
                    readOnly: true
              restartPolicy: Never
              automountServiceAccountToken: false
              volumes:
                - name: "request-body-volume" # placeholder
                  configMap:
                    name: registry-scheduler
---
# Source: kubescape-cloud-operator/templates/kubescape-scheduler/configmap.yaml
kind: ConfigMap 
apiVersion: v1 
metadata:
  name: kubescape-scheduler
  namespace: kubescape
  labels:
    app: kubescape-scheduler
    tier: ks-control-plane
data:
  request-body.json: |-
    {"commands":[{"CommandName":"kubescapeScan","args":{"scanV1": {}}}]}
---
# Source: kubescape-cloud-operator/templates/kubescape/configmap.yaml
kind: ConfigMap 
apiVersion: v1 
metadata:
  name: kubescape-config
  namespace: kubescape
  labels:
    app: kubescape-config
    tier: ks-control-plane  
data:
  config.json: |
    {
      "accountID": "2ce5daf4-e28d-4e6e-a239-03fda048070b",
      "clusterName": "arn-aws-eks-eu-west-2-015253967648-cluster-matan_avital-test-mjrwe",
      "clientID": "",
      "secretKey": ""
    }
---
# Source: kubescape-cloud-operator/templates/kubevuln-scheduler/configmap.yaml
kind: ConfigMap 
apiVersion: v1 
metadata:
  name: kubevuln-scheduler
  namespace: kubescape
  labels:
    app: kubevuln-scheduler
    tier: ks-control-plane
data:
  request-body.json: |-
    {"commands":[{"commandName":"scan","designators":[{"designatorType":"Attributes","attributes":{}}]}]}
---
# Source: kubescape-cloud-operator/templates/kubescape/clusterrole.yaml
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: kubescape-sa-roles
rules:
- apiGroups: ["*"]
  resources: ["*"]
  verbs: ["get", "list", "describe", "watch"]

# Host scan daemonset runs in dedicated namespace applied by kubescape at the begining of the scan.
# At the end of the resources collecting stage Kubescape is taking down both the namespace and the daemonset
- apiGroups: ["", "apps"]
  resources: ["namespaces", "daemonsets"]
  verbs: ["*"]
---
# Source: kubescape-cloud-operator/templates/kv-cluster-role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: ks-sa-roles
  namespace: kubescape
  labels:
    app: kubescape
rules:
- apiGroups: ["*"]
  resources: ["*"]
  verbs: ["get", "watch", "list", "describe"]
---
# Source: kubescape-cloud-operator/templates/kubescape/clusterrolebinding.yaml
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: kubescape-sa-role-binding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: kubescape-sa-roles
subjects:
- kind: ServiceAccount
  name: kubescape-sa
  namespace: kubescape
---
# Source: kubescape-cloud-operator/templates/kv-cluster-role-binding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: ks-sa-role-binding
  namespace: kubescape
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: ks-sa-roles
subjects:
- kind: ServiceAccount
  name: ks-sa
  namespace: kubescape
---
# Source: kubescape-cloud-operator/templates/ks-ns-role.yaml
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: ks-sa-roles
  namespace: kubescape
rules:
- apiGroups: ["*"]
  resources: ["*"]
  verbs: ["get", "list", "describe"]
- apiGroups: ["batch"]
  resources: ["cronjobs"]
  verbs: ["*"]
- apiGroups: [""]
  resources: ["configmaps", "secrets"]
  verbs: ["*"]
---
# Source: kubescape-cloud-operator/templates/ks-ns-role-binding.yaml
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: ks-sa-role-binding
  namespace: kubescape
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: ks-sa-roles
subjects:
- kind: ServiceAccount
  name: ks-sa
  namespace: kubescape
---
# Source: kubescape-cloud-operator/templates/gateway/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: gateway
  namespace: kubescape
  labels:
    app: gateway
spec:
  type: ClusterIP
  ports:
    - port: 8001
      targetPort: 8001
      protocol: TCP
      name: "websocket"
    - port: 8002
      targetPort: 8002
      protocol: TCP
      name: "http"
  selector:
    app: gateway
---
# Source: kubescape-cloud-operator/templates/kubescape/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: kubescape
  namespace: kubescape
  labels:
    app: kubescape
spec:
  type: ClusterIP
  ports:
    - name: http
      port: 8080
      targetPort: 8080
      protocol: TCP
  selector:
    app: kubescape
---
# Source: kubescape-cloud-operator/templates/kubevuln/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: kubevuln
  namespace: kubescape
  labels:
    app: kubevuln
spec:
  type: ClusterIP
  ports:
    - port: 8080
      targetPort: 8080
      protocol: TCP
      name: "vuln-scan-port"
    - port: 8000
      targetPort: 8000
      protocol: TCP
      name: "readiness-port"
  selector:
    app: kubevuln
---
# Source: kubescape-cloud-operator/templates/operator/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: operator
  namespace: kubescape
  labels:
    app: operator
spec:
  type: ClusterIP
  ports:
    - port: 4002
      targetPort: 4002
      protocol: TCP
  selector:
    app: operator
---
# Source: kubescape-cloud-operator/templates/otel-collector/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: otel-collector
  namespace: kubescape
  labels:
    app: otel-collector
spec:
  type: ClusterIP
  ports:
    - name: otlp
      port: 4317
      targetPort: 4317
      protocol: TCP
  selector:
    app: otel-collector
---
# Source: kubescape-cloud-operator/templates/gateway/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: gateway
  namespace: kubescape
  labels:
    app.kubernetes.io/name: gateway
    app.kubernetes.io/instance: kubescape
    app: gateway
    tier: ks-control-plane
    helm.sh/chart: kubescape-cloud-operator-1.10.17
spec:
  replicas: 1
  revisionHistoryLimit: 2
  strategy:
    rollingUpdate:
      maxSurge: 0%
      maxUnavailable: 100%
    type: RollingUpdate
  selector:
    matchLabels:
      app.kubernetes.io/name: gateway
      app.kubernetes.io/instance: kubescape
      tier: ks-control-plane
  template:
    metadata:
      labels:
        app.kubernetes.io/name: gateway
        app.kubernetes.io/instance: kubescape
        helm.sh/chart: kubescape-cloud-operator-1.10.17
        tier: ks-control-plane
        app: gateway
        helm.sh/revision: "3"
    spec:
      containers:
        - name: gateway
          image: "quay.io/kubescape/gateway:v0.1.11"
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            runAsNonRoot: true
            runAsUser: 100            
          livenessProbe:
            httpGet:
              path: /v1/liveness
              port: readiness-port
            initialDelaySeconds: 3
            periodSeconds: 3
          readinessProbe:
            httpGet:
              path: /v1/readiness
              port: readiness-port
            initialDelaySeconds: 10
            periodSeconds: 5
          ports:
            - name: "readiness-port"
              containerPort: 8000
              protocol: TCP
            - name: "websocket"
              containerPort: 8001
              protocol: TCP
            - name: "rest-api"
              containerPort: 8002
              protocol: TCP
          resources:
            limits:
              cpu: 100m
              memory: 50Mi
            requests:
              cpu: 10m
              memory: 10Mi
          env: 
            - name: KS_LOGGER_LEVEL
              value: "info"
            - name: KS_LOGGER_NAME
              value: "zap" 
            - name: WEBSOCKET_PORT
              value: "8001"
            - name: HTTP_PORT
              value: "8002"
            - name: ACCOUNT_ID
              value: "2ce5daf4-e28d-4e6e-a239-03fda048070b"
            - name: OTEL_COLLECTOR_SVC
              value: "otel-collector:4317"
            - name: HTTPS_PROXY
              value: "https://nginx-proxy.default:443"
            - name : no_proxy
              value: "kubescape,kubevuln,operator,otel-collector,kubernetes.default.svc.*"
          args:
            - -alsologtostderr
            - -v=4
            - 2>&1
          volumeMounts:
          - name: ks-cloud-config
            mountPath: /etc/config
            readOnly: true
      volumes:
        - name: ks-cloud-config
          configMap:
            name: ks-cloud-config
            items:
            - key: "clusterData"
              path: "clusterData.json"
      automountServiceAccountToken: false
---
# Source: kubescape-cloud-operator/templates/kubescape/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kubescape
  namespace: kubescape
  labels:
    app.kubernetes.io/name: kubescape
    app.kubernetes.io/instance: kubescape
    app: kubescape
    tier: ks-control-plane
    helm.sh/chart: kubescape-cloud-operator-1.10.17
spec:
  replicas: 1
  revisionHistoryLimit: 2
  strategy:
    rollingUpdate:
      maxSurge: 0%
      maxUnavailable: 100%
    type: RollingUpdate
  selector:
    matchLabels:
      app.kubernetes.io/name: kubescape
      app.kubernetes.io/instance: kubescape
      tier: ks-control-plane
  template:
    metadata:
      labels:
        app.kubernetes.io/name: kubescape
        app.kubernetes.io/instance: kubescape
        helm.sh/chart: kubescape-cloud-operator-1.10.17
        tier: ks-control-plane
        app: kubescape
        otel: enabled
        helm.sh/revision: "3"
    spec:
      containers:
      - name: kubescape
        image: "quay.io/kubescape/kubescape:v2.3.3"
        imagePullPolicy: "Always"
        securityContext:
          allowPrivilegeEscalation: false
          runAsNonRoot: true
          runAsUser: 100
        ports:
          - name: http
            containerPort: 8080
            protocol: TCP
        livenessProbe:
          httpGet:
            path: /livez
            port: 8080
          initialDelaySeconds: 3
          periodSeconds: 3
        readinessProbe:
          httpGet:
            path: /readyz
            port: 8080
          initialDelaySeconds: 3
          periodSeconds: 3
        env:
        - name: CLUSTER_NAME
          value: "arn-aws-eks-eu-west-2-015253967648-cluster-matan_avital-test-mjrwe"
        - name: KS_LOGGER_LEVEL
          value: "info"
        - name: KS_LOGGER_NAME
          value: "zap"
        - name: KS_DOWNLOAD_ARTIFACTS  # When set to true the artifacts will be downloaded every scan execution
          value: "true" 
        - name: RULE_PROCESSING_GOMAXPROCS
          value: "1"
        - name: KS_DEFAULT_CONFIGMAP_NAME
          value: "kubescape-config"
        - name: KS_DEFAULT_CONFIGMAP_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: KS_CONTEXT
          value: "arn:aws:eks:eu-west-2:015253967648:cluster/matan_avital-test-mjrwe"
        - name: KS_ENABLE_HOST_SCANNER
          value: "true"
        - name: KS_SUBMIT
          value: "true"
        - name: KS_SKIP_UPDATE_CHECK
          value: "false"
        - name: KS_HOST_SCAN_YAML
          value: "/home/ks/.kubescape/host-scanner.yaml"
        - name: KS_SAAS_ENV
        
          value: "dev"
        - name: ACCOUNT_ID
          value: "2ce5daf4-e28d-4e6e-a239-03fda048070b"
        - name: OTEL_COLLECTOR_SVC
          value: "otel-collector:4317"
        - name: HTTPS_PROXY
          value: "https://nginx-proxy.default:443"
        - name : no_proxy
          value: "kubescape,kubevuln,operator,otel-collector,kubernetes.default.svc.*"
        command:
        - ksserver
        resources:
              limits:
                cpu: 600m
                memory: 800Mi
              requests:
                cpu: 250m
                memory: 400Mi            
        volumeMounts:
        - name: kubescape-config-volume
          mountPath: /home/ks/.kubescape/config.json
          subPath: config.json
        - name: host-scanner-definition
          mountPath: /home/ks/.kubescape/host-scanner.yaml
          subPath: host-scanner-yaml
      serviceAccountName: kubescape-sa
      automountServiceAccountToken: true
      volumes:
      - name: kubescape-config-volume
        configMap:
          name: kubescape-config
      - name: host-scanner-definition
        configMap:
          name: host-scanner-definition
---
# Source: kubescape-cloud-operator/templates/kubevuln/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kubevuln
  namespace: kubescape
  labels:
    app.kubernetes.io/name: kubevuln
    app.kubernetes.io/instance: kubescape
    app: kubevuln
    tier: ks-control-plane
spec:
  replicas: 1
  revisionHistoryLimit: 2
  strategy:
    rollingUpdate:
      maxSurge: 0%
      maxUnavailable: 100%
    type: RollingUpdate
  selector:
    matchLabels:
      app.kubernetes.io/name: kubevuln
      app.kubernetes.io/instance: kubescape
      tier: ks-control-plane
  template:
    metadata:
      labels:
        app.kubernetes.io/name: kubevuln
        app.kubernetes.io/instance: kubescape
        helm.sh/chart: kubescape-cloud-operator-1.10.17
        tier: ks-control-plane
        app: kubevuln
        otel: enabled
        helm.sh/revision: "3"
    spec:
      containers:
        - name: kubevuln
          image: "quay.io/kubescape/kubevuln:v0.1.19"
          imagePullPolicy: Always
          securityContext:
            allowPrivilegeEscalation: false
            runAsNonRoot: true
            runAsUser: 100
          ports:
          - name: "vuln-scan-port"
            containerPort: 8080
            protocol: TCP
          - name: "readiness-port"
            containerPort: 8000
            protocol: TCP
          livenessProbe:
            httpGet:
              path: /v1/liveness
              port: readiness-port
            initialDelaySeconds: 3
            periodSeconds: 3
          readinessProbe:
            httpGet:
              path: /v1/readiness
              port: readiness-port
          resources:
            limits:
              cpu: 500m
              memory: 5000Mi
            requests:
              cpu: 300m
              ephemeral-storage: 5Gi
              memory: 1000Mi
          env:
            - name: KS_LOGGER_LEVEL
              value: "info"
            - name: KS_LOGGER_NAME
              value: "zap"
            - name: PRINT_POST_JSON
              value: ""
            - name: CA_MAX_VULN_SCAN_ROUTINES
              value: "1"
            - name: ACCOUNT_ID
              value: "2ce5daf4-e28d-4e6e-a239-03fda048070b"
            - name: OTEL_COLLECTOR_SVC
              value: "otel-collector:4317"
            - name: HTTPS_PROXY
              value: "https://nginx-proxy.default:443"
            - name : no_proxy
              value: "kubescape,kubevuln,operator,otel-collector,kubernetes.default.svc.*"
          args:
            - -alsologtostderr
            - -v=4
            - 2>&1
          volumeMounts:
            - name: ks-cloud-config
              mountPath: /etc/config
              readOnly: true
      volumes:
        - name: ks-cloud-config
          configMap:
            name: ks-cloud-config
            items:
            - key: "clusterData"
              path: "clusterData.json"
      serviceAccountName: ks-sa
      automountServiceAccountToken: true
---
# Source: kubescape-cloud-operator/templates/operator/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: operator
  namespace: kubescape
  labels:
    app.kubernetes.io/name: operator
    app.kubernetes.io/instance: kubescape
    app: operator
    tier: ks-control-plane
spec:
  replicas: 1
  revisionHistoryLimit: 2
  strategy:
    rollingUpdate:
      maxSurge: 0%
      maxUnavailable: 100%
    type: RollingUpdate
  selector:
    matchLabels:
      app.kubernetes.io/name: operator
      app.kubernetes.io/instance: kubescape
      tier: ks-control-plane
  template:
    metadata:
      labels:
        app.kubernetes.io/name: operator
        app.kubernetes.io/instance: kubescape
        helm.sh/chart: kubescape-cloud-operator-1.10.17
        tier: ks-control-plane
        app: operator
        otel: enabled
        helm.sh/revision: "3"
    spec:
      containers:
        - name: operator
          image: "quay.io/kubescape/operator:v0.1.18"
          imagePullPolicy: Always
          securityContext:
            allowPrivilegeEscalation: false
            runAsNonRoot: true
            runAsUser: 100
          ports:
            - name: "trigger-port"
              containerPort: 4002
              protocol: TCP
            - name: "readiness-port"
              containerPort: 8000
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /v1/liveness
              port: readiness-port
            initialDelaySeconds: 3
            periodSeconds: 3
          readinessProbe:
            httpGet:
              path: /v1/readiness
              port: readiness-port
            initialDelaySeconds: 10
            periodSeconds: 5
          resources:
            limits:
              cpu: 300m
              memory: 300Mi
            requests:
              cpu: 50m
              memory: 100Mi
          env:
            - name: KS_LOGGER_LEVEL
              value: "info"
            - name: KS_LOGGER_NAME
              value: "zap"
            - name: NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: ACCOUNT_ID
              value: "2ce5daf4-e28d-4e6e-a239-03fda048070b"
            - name: OTEL_COLLECTOR_SVC
              value: "otel-collector:4317"
            - name: HTTPS_PROXY
              value: "https://nginx-proxy.default:443"
            - name : no_proxy
              value: "kubescape,kubevuln,operator,otel-collector,kubernetes.default.svc.*"
          args:
            - -alsologtostderr
            - -v=4
            - 2>&1
          volumeMounts:
            - name: ks-cloud-config
              mountPath: /etc/config
              readOnly: true
      volumes:
        - name: ks-cloud-config
          configMap:
            name: ks-cloud-config
            items:
            - key: "clusterData"
              path: "clusterData.json"
      serviceAccountName: ks-sa
      automountServiceAccountToken: true
---
# Source: kubescape-cloud-operator/templates/otel-collector/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: otel-collector
  namespace: kubescape
  labels:
    app.kubernetes.io/name: otel-collector
    app.kubernetes.io/instance: kubescape
    app: otel-collector
    tier: ks-control-plane
    helm.sh/chart: kubescape-cloud-operator-1.10.17
spec:
  replicas: 1
  revisionHistoryLimit: 2
  strategy:
    rollingUpdate:
      maxSurge: 0%
      maxUnavailable: 100%
    type: RollingUpdate
  selector:
    matchLabels:
      app.kubernetes.io/name: otel-collector
      app.kubernetes.io/instance: kubescape
      tier: ks-control-plane
  template:
    metadata:
      labels:
        app.kubernetes.io/name: otel-collector
        app.kubernetes.io/instance: kubescape
        helm.sh/chart: kubescape-cloud-operator-1.10.17
        tier: ks-control-plane
        app: otel-collector
        helm.sh/revision: "3"
    spec:
      containers:
      - name: otel-collector
        image: "otel/opentelemetry-collector:0.70.0"
        imagePullPolicy: "Always"
        securityContext:
          allowPrivilegeEscalation: false
          runAsNonRoot: true
          runAsUser: 100
        ports:
          - name: otlp
            containerPort: 4317
            protocol: TCP
        env:
        - name: GOGC
          value: "80"
        - name: HTTPS_PROXY
          value: "https://nginx-proxy.default:443"
        - name : no_proxy
          value: "kubescape,kubevuln,operator,otel-collector,kubernetes.default.svc.*"
        command:
        - "/otelcol"
        - "--config=/conf/otel-collector-config.yaml"
        resources:
              limits:
                cpu: 1
                memory: 1Gi
              requests:
                cpu: 100m
                memory: 200Mi            
        volumeMounts:
        - name: otel-collector-config-volume
          mountPath: /conf
      serviceAccountName: kubescape-sa
      automountServiceAccountToken: true
      volumes:
      - name: otel-collector-config-volume
        configMap:
          name: otel-collector-config
---
# Source: kubescape-cloud-operator/templates/kollector/statefulset.yaml
apiVersion: apps/v1
# statefulset is needed in order to avoid two pods reporting from the same cluster in parallel.
# parallel reporting will cause Kubescape SaaS to miss identify the cluster liveness status
kind: StatefulSet
metadata:
  name: kollector
  namespace: kubescape
  labels:
    app.kubernetes.io/name: kollector
    app.kubernetes.io/instance: kubescape
    app: kollector
    tier: ks-control-plane
  # annotations:
  #   helm.sh/hook: pre-install,pre-upgrade # hook should apply when installing and upgrading
  #   helm.sh/hook-weight: "-1" # run before all other hooks
  #   helm.sh/hook-delete-policy: before-hook-creation #,hook-succeeded
spec:
  serviceName: ""
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: kollector
      app.kubernetes.io/instance: kubescape
      tier: ks-control-plane
  template:
    metadata:
      labels:
        app.kubernetes.io/name: kollector
        app.kubernetes.io/instance: kubescape
        tier: ks-control-plane
        app: kollector
        helm.sh/chart: kubescape-cloud-operator-1.10.17
        helm.sh/revision: "3"
    spec:
      containers:
        - name: kollector
          image: "quay.io/kubescape/kollector:v0.1.16"
          imagePullPolicy: Always
          securityContext:
            allowPrivilegeEscalation: false
            runAsNonRoot: true
            runAsUser: 100
          ports:
            - name: "readiness-port"
              containerPort: 8000
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /v1/liveness
              port: readiness-port
            initialDelaySeconds: 3
            periodSeconds: 3
          readinessProbe:
            httpGet:
              path: /v1/readiness
              port: readiness-port
            initialDelaySeconds: 10
            periodSeconds: 5
          resources:
            limits:
              cpu: 500m
              memory: 500Mi
            requests:
              cpu: 10m
              memory: 40Mi
          env:
            - name: KS_LOGGER_LEVEL
              value: "info"
            - name: KS_LOGGER_NAME
              value: "zap"
            - name: NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: ACCOUNT_ID
              value: "2ce5daf4-e28d-4e6e-a239-03fda048070b"
            - name: OTEL_COLLECTOR_SVC
              value: "otel-collector:4317"
            - name: HTTPS_PROXY
              value: "https://nginx-proxy.default:443"
            - name : no_proxy
              value: "kubescape,kubevuln,operator,otel-collector,kubernetes.default.svc.*"
            - name: PRINT_REPORT
              value: "false"
            - name: WAIT_BEFORE_REPORT
              value: "0"
          args:
          - -alsologtostderr
          - -v=4
          - 2>&1
          volumeMounts:
            - name: ks-cloud-config
              mountPath: /etc/config
              readOnly: true
      volumes:
        - name: ks-cloud-config
          configMap:
            name: ks-cloud-config
            items:
            - key: "clusterData"
              path: "clusterData.json"
      serviceAccountName: ks-sa
      automountServiceAccountToken: true
---
# Source: kubescape-cloud-operator/templates/kubescape-scheduler/cronjob.yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: kubescape-scheduler
  namespace: kubescape
  labels:
    app.kubernetes.io/name: kubescape-scheduler
    app: kubescape-scheduler
    tier: ks-control-plane
    armo.tier: "kubescape-scan"
spec:
  schedule: "21 7 * * *"
  jobTemplate:
    spec:
      template:
        metadata:
          labels:
            app.kubernetes.io/name: kubescape-scheduler
            app: kubescape-scheduler
            armo.tier: "kubescape-scan"
        spec:
          containers:
          - name: kubescape-scheduler
            image: "quay.io/kubescape/http-request:v0.0.14"
            imagePullPolicy: IfNotPresent
            resources:
              limits:
                cpu: 10m
                memory: 20Mi
              requests:
                cpu: 1m
                memory: 10Mi
            args: 
              - -method=post
              - -scheme=http
              - -host=operator:4002
              - -path=v1/triggerAction
              - -headers="Content-Type:application/json"
              - -path-body=/home/ks/request-body.json
            volumeMounts:
              - name: kubescape-scheduler
                mountPath: /home/ks/request-body.json
                subPath: request-body.json
                readOnly: true
          restartPolicy: Never
          automountServiceAccountToken: false
          volumes:
          - name: kubescape-scheduler
            configMap:
              name: kubescape-scheduler
---
# Source: kubescape-cloud-operator/templates/kubevuln-scheduler/cronjob.yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: kubevuln-scheduler
  namespace: kubescape
  labels:
    app.kubernetes.io/name: kubevuln-scheduler
    app: kubevuln-scheduler
    tier: ks-control-plane
    armo.tier: "vuln-scan"
spec:
  schedule: "22 6 * * *"
  jobTemplate:
    spec:
      template:
        metadata:
          labels:
            app.kubernetes.io/name: kubevuln-scheduler
            app: kubevuln-scheduler
            armo.tier: "vuln-scan"
        spec:
          containers:
          - name: kubevuln-scheduler
            image: "quay.io/kubescape/http-request:v0.0.14"
            imagePullPolicy: IfNotPresent
            resources:
              limits:
                cpu: 10m
                memory: 20Mi
              requests:
                cpu: 1m
                memory: 10Mi
            args: 
              - -method=post
              - -scheme=http
              - -host=operator:4002
              - -path=v1/triggerAction
              - -headers="Content-Type:application/json"
              - -path-body=/home/ks/request-body.json
            volumeMounts:
              - name: kubevuln-scheduler
                mountPath: /home/ks/request-body.json
                subPath: request-body.json
                readOnly: true
          restartPolicy: Never
          automountServiceAccountToken: false
          volumes:
          - name: kubevuln-scheduler
            configMap:
              name: kubevuln-scheduler

NOTES:
Thank you for installing kubescape-cloud-operator version 1.10.17.

You can see and change the values of your's recurring configurations daily scan in the following link:
https://cloud.armosec.io/settings/assets/clusters/scheduled-scans?cluster=arn-aws-eks-eu-west-2-015253967648-cluster-matan_avital-test-mjrwe
> kubectl -n kubescape get cj kubescape-scheduler -o=jsonpath='{.metadata.name}{"\t"}{.spec.schedule}{"\n"}'

You can see and change the values of your's recurring images daily scan in the following link:
https://cloud.armosec.io/settings/assets/images
> kubectl -n kubescape get cj kubevuln-scheduler -o=jsonpath='{.metadata.name}{"\t"}{.spec.schedule}{"\n"}'

See you!!!
